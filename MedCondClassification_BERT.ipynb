{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ypw2tW0QFU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcb0a09-73e5-447c-be1c-b223f03addbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "from transformers import AdamW, BertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef-Tt3WxSes9",
        "outputId": "d1bee373-a2cf-4ef9-f745-164f3c286b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: piyushhinduja\n",
            "Your Kaggle Key: ··········\n",
            "Downloading kuc-hackathon-winter-2018.zip to ./kuc-hackathon-winter-2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40.7M/40.7M [00:00<00:00, 74.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_SIZE = 300\n",
        "BERT_MODEL = 'prajjwal1/bert-mini'\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 10\n",
        "\n",
        "kaggle_train = pd.read_csv('/content/kuc-hackathon-winter-2018/drugsComTrain_raw.csv')\n",
        "kaggle_test = pd.read_csv('/content/kuc-hackathon-winter-2018/drugsComTest_raw.csv')\n",
        "\n",
        "main_x = list(pd.concat([kaggle_train['review'], kaggle_test['review']], axis=0, ignore_index=True))\n",
        "main_y = list(pd.concat([kaggle_train['condition'], kaggle_test['condition']], axis=0))\n",
        "\n",
        "main_x = main_x[:500]\n",
        "main_y = main_y[:500]\n",
        "\n",
        "vocab = list(Counter(main_y).keys())\n",
        "i_to_x = {i:vocab[i] for i in range(len(vocab))}\n",
        "x_to_i = {vocab[i]:i for i in range(len(vocab))}\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(main_x, main_y, test_size=.21, random_state=0)\n",
        "x_test, x_val,  y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=0)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "x_train = tokenizer(x_train, truncation=True, max_length=512, padding=True, return_tensors='pt').to(device)\n",
        "y_train = [x_to_i[j] for j in y_train]\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "train_dataset = TensorDataset(x_train['input_ids'], x_train['attention_mask'], x_train['token_type_ids'], y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "x_test = tokenizer(x_test, truncation=True, max_length=512, padding=True, return_tensors='pt').to(device)\n",
        "y_test = [x_to_i[j] for j in y_test]\n",
        "y_test = torch.tensor(y_test).to(device)\n",
        "test_dataset = TensorDataset(x_test['input_ids'], x_test['attention_mask'], x_test['token_type_ids'], y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "x_val = tokenizer(x_val, truncation=True, max_length=512, padding=True, return_tensors='pt').to(device)\n",
        "y_val = [x_to_i[j] for j in y_val]\n",
        "y_val = torch.tensor(y_val).to(device)\n",
        "val_dataset = TensorDataset(x_val['input_ids'], x_val['attention_mask'], x_val['token_type_ids'], y_val)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "Zx4Wikv1Syl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class DrugClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DrugClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
        "    self.lin1 = nn.Linear(self.bert.config.hidden_size, 512)\n",
        "    self.lin2 = nn.Linear(512, len(vocab))\n",
        "    # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "    output1 = self.lin1(pooled_output)\n",
        "    output2 = self.lin2(output1)\n",
        "    # return self.softmax(output)\n",
        "    return output2\n",
        "\n",
        "model = DrugClassifier().to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "best_model = {'accuracy':-1, 'epoch':-1, 'model':{}, 'optimizer':{}}\n",
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch: ', epoch+1)\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  f1_scores = []\n",
        "  for input_ids, attention_mask, token_type_ids, labels in tqdm(train_dataloader):\n",
        "    model.train()\n",
        "    out = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "    loss = loss_func(out, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    pred = torch.max(out, dim=1, keepdim=True)[1]\n",
        "    pred = pred.view(pred.shape[0]).to(torch.float32).to(device)\n",
        "    acc = accuracy_score(pred.tolist(), labels.tolist())\n",
        "    accuracies.append(acc.item())\n",
        "    f1 = f1_score(pred.tolist(), labels.tolist(), average='weighted')\n",
        "    f1_scores.append(f1.item())\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  print('Train Loss: ', sum(losses)/len(losses))\n",
        "  print('Train Accuracy: ', sum(accuracies)/len(accuracies))\n",
        "  print('Train F1 score: ', sum(f1_scores)/len(f1_scores))\n",
        "\n",
        "  val_accuracies = []\n",
        "  val_losses = []\n",
        "  val_f1 = []\n",
        "  with torch.no_grad():\n",
        "    for input_ids, attention_mask, token_type_ids, labels in tqdm(val_dataloader):\n",
        "      model.eval()\n",
        "      pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      loss = loss_func(pred, labels)\n",
        "      pred = torch.max(pred, dim=1, keepdim=True)[1]\n",
        "      pred = pred.view(pred.shape[0]).to(torch.float32)\n",
        "      acc = accuracy_score(pred.tolist(), labels.tolist())\n",
        "      val_accuracies.append(acc.item())\n",
        "      f1 = f1_score(pred.tolist(), labels.tolist(), average='weighted')\n",
        "      val_f1.append(f1.item())\n",
        "      val_losses.append(loss.item())\n",
        "    print('Dev Loss: ', sum(val_losses)/len(val_losses))\n",
        "    print('Dev Accuracy: ', sum(val_accuracies)/len(val_accuracies))\n",
        "    print('Dev F1 score: ', sum(val_f1)/len(val_f1))\n",
        "\n",
        "  if best_model['accuracy'] < sum(val_accuracies)/len(val_accuracies):\n",
        "    best_model['accuracy'] = sum(val_accuracies)/len(val_accuracies)\n",
        "    best_model['epoch'] = epoch+1\n",
        "    best_model['model'] = model.state_dict()\n",
        "    best_model['optimizer'] = optimizer.state_dict()\n",
        "\n",
        "torch.save({\n",
        "    'accuracy':best_model['accuracy'],\n",
        "    'epoch':best_model['epoch'],\n",
        "    'model':best_model['model'],\n",
        "    'optimizer':best_model['optimizer']\n",
        "}, './best_model6')"
      ],
      "metadata": {
        "id": "T7Aw9FotRZI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './best_model6'\n",
        "checkpoint = torch.load(model_path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "test_accuracies = []\n",
        "test_f1_scores = []\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, token_type_ids, labels in tqdm(val_dataloader):\n",
        "      model.eval()\n",
        "      pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      loss = loss_func(pred, labels)\n",
        "      pred = torch.max(pred, dim=1, keepdim=True)[1]\n",
        "      pred = pred.view(pred.shape[0]).to(torch.float32)\n",
        "      acc = accuracy_score(pred.tolist(), labels.tolist())\n",
        "      test_accuracies.append(acc.item())\n",
        "      f1 = f1_score(pred.tolist(), labels.tolist(), average='weighted')\n",
        "      test_f1_scores.append(f1.item())\n",
        "      test_losses.append(loss.item())\n",
        "\n",
        "  print('Test Loss: ', sum(test_losses)/len(test_losses))\n",
        "  print('Test F1 score: ', sum(test_f1_scores)/len(test_f1_scores))\n",
        "  print('Test Accuracy: ', sum(test_accuracies)/len(test_accuracies))"
      ],
      "metadata": {
        "id": "jxYgB_v4S1J4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
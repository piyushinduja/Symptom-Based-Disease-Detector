{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut6AjWtNrSCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d232df-a601-403a-c976-b6d06e853a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVzEFOH4roPi",
        "outputId": "d7d54235-a289-44b2-dcf7-e27a7b12c6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: piyushhinduja\n",
            "Your Kaggle Key: ··········\n",
            "Downloading kuc-hackathon-winter-2018.zip to ./kuc-hackathon-winter-2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40.7M/40.7M [00:02<00:00, 14.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_SIZE = 300\n",
        "# BERT_MODEL = 'prajjwal1/bert-mini'\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 10\n",
        "glove = GloVe(name='6B', dim=EMBED_SIZE)\n",
        "\n",
        "kaggle_train = pd.read_csv('/content/kuc-hackathon-winter-2018/drugsComTrain_raw.csv')\n",
        "kaggle_test = pd.read_csv('/content/kuc-hackathon-winter-2018/drugsComTest_raw.csv')\n",
        "\n",
        "main_x = list(pd.concat([kaggle_train['review'], kaggle_test['review']], axis=0, ignore_index=True))\n",
        "main_y = list(pd.concat([kaggle_train['condition'], kaggle_test['condition']], axis=0))\n",
        "\n",
        "main_x = main_x[:500]\n",
        "main_y = main_y[:500]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(main_x, main_y, test_size=.21, random_state=0)\n",
        "x_test, x_val,  y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=0)\n",
        "\n",
        "vocab = list(Counter(main_y).keys())\n",
        "i_to_x = {i:vocab[i] for i in range(len(vocab))}\n",
        "x_to_i = {vocab[i]:i for i in range(len(vocab))}\n",
        "\n",
        "max_label = x_to_i[vocab[0]]\n",
        "\n",
        "def create_data(x, y):\n",
        "  k = 0\n",
        "  while True:\n",
        "    if k == len(x):\n",
        "      break\n",
        "    if len(x[k].split(' ')) > 500:\n",
        "      x.pop(k)\n",
        "      y.pop(k)\n",
        "      k += 1\n",
        "      continue\n",
        "    k += 1\n",
        "  last_indices = []\n",
        "  tokens = []\n",
        "  for i in range(len(x)):\n",
        "    tokens.append(x[i].split(' '))\n",
        "    last_indices.append(len(tokens[i])-1)\n",
        "    pad_len = 500 - len(tokens[i]) % 500\n",
        "    tokens[i].extend(['[PAD]']*pad_len)\n",
        "\n",
        "  embeddings = []\n",
        "  for i in range(len(tokens)):\n",
        "    sent_embed = []\n",
        "    for j in range(500):\n",
        "      sent_embed.append(glove[tokens[i][j].lower()].tolist())\n",
        "    embeddings.append(sent_embed)\n",
        "\n",
        "  x_tensor = torch.tensor(embeddings).to(device)\n",
        "  print(x_tensor.shape)\n",
        "  y_tensor = torch.tensor([x_to_i[j] for j in y], dtype=torch.long).to(device)\n",
        "  print(y_tensor.shape)\n",
        "  last_indices = torch.tensor(last_indices).to(device)\n",
        "  print(last_indices.shape)\n",
        "  dataset = TensorDataset(x_tensor, y_tensor, last_indices)\n",
        "  dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "train_dataloader = create_data(x_train, y_train)\n",
        "test_dataloader = create_data(x_test, y_test)\n",
        "val_dataloader = create_data(x_val, y_val)"
      ],
      "metadata": {
        "id": "wEsJroncracC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b10259-0d81-4ba9-a4ed-a3fa7ed148f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([395, 500, 300])\n",
            "torch.Size([395])\n",
            "torch.Size([395])\n",
            "torch.Size([52, 500, 300])\n",
            "torch.Size([52])\n",
            "torch.Size([52])\n",
            "torch.Size([53, 500, 300])\n",
            "torch.Size([53])\n",
            "torch.Size([53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.1\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True)\n",
        "    self.lin1 = nn.Linear(300, VOCAB_SIZE)\n",
        "\n",
        "  def forward(self, x, indices):\n",
        "    out, (h, c) = self.lstm(x)\n",
        "    out1 = torch.reshape(out, (out.shape[0]*out.shape[1], out.shape[2]))\n",
        "    out1 = torch.index_select(out1, -2, indices)\n",
        "    l1 = self.lin1(out1)\n",
        "    return l1\n",
        "\n",
        "model = LSTM().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "best_model = {'accuracy':-1, 'epoch':-1, 'model':{}, 'optimizer':{}}\n",
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch:', epoch+1)\n",
        "  losses = []\n",
        "  f1_scores = []\n",
        "  accuracies = []\n",
        "  max_label_accuracies = []\n",
        "  for x, y, indices in tqdm(train_dataloader):\n",
        "    model.train()\n",
        "    pred = model(x, indices)\n",
        "    loss = loss_func(pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    pred = torch.max(pred, dim=1, keepdim=True)[1]\n",
        "    pred = pred.view(pred.shape[0]).to(torch.float32).to(device)\n",
        "    f1 = f1_score(pred.tolist(), y.tolist(), average='weighted')\n",
        "    f1_scores.append(f1.item())\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Accuracy with original predictions\n",
        "    acc = accuracy_score(pred.tolist(), y.tolist())\n",
        "    accuracies.append(acc.item())\n",
        "\n",
        "    # Max label accuracy\n",
        "    max_label_list = [max_label]*len(y.tolist())\n",
        "    acc1 = accuracy_score(max_label_list, y.tolist())\n",
        "    max_label_accuracies.append(acc1.item())\n",
        "\n",
        "\n",
        "  print('Train Loss: ', sum(losses)/len(losses))\n",
        "  print('Train F1 score: ', sum(f1_scores)/len(f1_scores))\n",
        "  print('Train accuracy: ', sum(accuracies)/len(accuracies))\n",
        "  print('Train Max label accuracy: ', sum(max_label_accuracies)/len(max_label_accuracies))\n",
        "\n",
        "  val_accuracies = []\n",
        "  val_f1_scores = []\n",
        "  val_max_label_acc = []\n",
        "  val_losses = []\n",
        "  with torch.no_grad():\n",
        "    for x, y, indices in tqdm(val_dataloader):\n",
        "      model.eval()\n",
        "      pred = model(x, indices)\n",
        "      loss = loss_func(pred, y)\n",
        "      pred = torch.max(pred, dim=1, keepdim=True)[1]\n",
        "      pred = pred.view(pred.shape[0]).to(torch.float32)\n",
        "      f1 = f1_score(pred.tolist(), y.tolist(), average='weighted')\n",
        "      val_f1_scores.append(f1.item())\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "      # Accuracy with original predictions\n",
        "      acc = accuracy_score(pred.tolist(), y.tolist())\n",
        "      val_accuracies.append(acc.item())\n",
        "\n",
        "      # Max label accuracy\n",
        "      max_label_list = [max_label]*len(y.tolist())\n",
        "      acc1 = accuracy_score(max_label_list, y.tolist())\n",
        "      val_max_label_acc.append(acc1.item())\n",
        "\n",
        "    print('Dev Loss: ', sum(val_losses)/len(val_losses))\n",
        "    print('Dev F1 score: ', sum(val_f1_scores)/len(val_f1_scores))\n",
        "    print('Dev Accuracy: ', sum(val_accuracies)/len(val_accuracies))\n",
        "    print('Dev Max label accuracy: ', sum(val_max_label_acc)/len(val_max_label_acc))\n",
        "\n",
        "  if best_model['accuracy'] < sum(val_accuracies)/len(val_accuracies):\n",
        "    best_model['accuracy'] = sum(val_accuracies)/len(val_accuracies)\n",
        "    best_model['epoch'] = epoch+1\n",
        "    best_model['model'] = model.state_dict()\n",
        "    best_model['optimizer'] = optimizer.state_dict()\n",
        "\n",
        "torch.save({\n",
        "    'accuracy':best_model['accuracy'],\n",
        "    'epoch':best_model['epoch'],\n",
        "    'model':best_model['model'],\n",
        "    'optimizer':best_model['optimizer']\n",
        "}, './best_model6')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olyB-AMerj-r",
        "outputId": "34a23bdb-6512-49ef-eeb2-4569a4ae1a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 14.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  9.101677894592285\n",
            "Train F1 score:  0.07330733589850792\n",
            "Train accuracy:  0.05539772727272728\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 41.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  12.199882507324219\n",
            "Dev F1 score:  0.25157232704402516\n",
            "Dev Accuracy:  0.1509433962264151\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  11.72543716430664\n",
            "Train F1 score:  0.053431839144309934\n",
            "Train accuracy:  0.03125\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 41.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  14.241783142089844\n",
            "Dev F1 score:  0.0\n",
            "Dev Accuracy:  0.0\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  10.420822416033063\n",
            "Train F1 score:  0.1068918535568916\n",
            "Train accuracy:  0.07589285714285714\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 46.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  16.42892837524414\n",
            "Dev F1 score:  0.03689727463312369\n",
            "Dev Accuracy:  0.018867924528301886\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  9.143024103982109\n",
            "Train F1 score:  0.14068067873175494\n",
            "Train accuracy:  0.09334415584415585\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 45.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  12.390040397644043\n",
            "Dev F1 score:  0.036163522012578615\n",
            "Dev Accuracy:  0.018867924528301886\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  7.7620954513549805\n",
            "Train F1 score:  0.07970572082800463\n",
            "Train accuracy:  0.049107142857142856\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 45.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  11.466385841369629\n",
            "Dev F1 score:  0.24528301886792453\n",
            "Dev Accuracy:  0.1509433962264151\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  7.045139040265765\n",
            "Train F1 score:  0.1254818463246133\n",
            "Train accuracy:  0.078125\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 41.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  10.266172409057617\n",
            "Dev F1 score:  0.24407868325973503\n",
            "Dev Accuracy:  0.1509433962264151\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 18.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  6.32019989831107\n",
            "Train F1 score:  0.27041803800715974\n",
            "Train accuracy:  0.17694805194805197\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 47.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  8.796109199523926\n",
            "Dev F1 score:  0.06659267480577136\n",
            "Dev Accuracy:  0.05660377358490566\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  5.974339144570487\n",
            "Train F1 score:  0.2128441543295802\n",
            "Train accuracy:  0.13717532467532467\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  8.93282413482666\n",
            "Dev F1 score:  0.28692257644762525\n",
            "Dev Accuracy:  0.16981132075471697\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 18.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  5.887031214577811\n",
            "Train F1 score:  0.14656746712511298\n",
            "Train accuracy:  0.09375\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 44.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  8.209282875061035\n",
            "Dev F1 score:  0.012578616352201257\n",
            "Dev Accuracy:  0.018867924528301886\n",
            "Dev Max label accuracy:  0.0\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 19.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  5.57134062903268\n",
            "Train F1 score:  0.1622804938779002\n",
            "Train accuracy:  0.10267857142857142\n",
            "Train Max label accuracy:  0.002232142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 47.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Loss:  7.546858787536621\n",
            "Dev F1 score:  0.2372431715438131\n",
            "Dev Accuracy:  0.1509433962264151\n",
            "Dev Max label accuracy:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './best_model6'\n",
        "checkpoint = torch.load(model_path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "test_accuracies = []\n",
        "test_f1_scores = []\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "  for x, y, indices in tqdm(test_dataloader):\n",
        "      model.eval()\n",
        "      pred = model(x, indices)\n",
        "      loss = loss_func(pred, y)\n",
        "      pred = torch.max(pred, dim=1, keepdim=True)[1]\n",
        "      pred = pred.view(pred.shape[0]).to(torch.float32)\n",
        "      f1 = f1_score(pred.tolist(), y.tolist(), average='weighted')\n",
        "      val_f1_scores.append(f1.item())\n",
        "      val_losses.append(loss.item())\n",
        "      acc = accuracy_score(pred.tolist(), y.tolist())\n",
        "      val_accuracies.append(acc.item())\n",
        "\n",
        "  print('Test Loss: ', sum(val_losses)/len(val_losses))\n",
        "  print('Test F1 score: ', sum(val_f1_scores)/len(val_f1_scores))\n",
        "  print('Test Accuracy: ', sum(val_accuracies)/len(val_accuracies))"
      ],
      "metadata": {
        "id": "G0rgKSsSchHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}